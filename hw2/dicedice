#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict

# run code with different initialization

optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float",
                     help="Threshold for aligning with Dice's coefficient (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=2, type="int",
                     help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()
f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

# corpus

bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sents]

# initialize dictionaries for foreign and english word count, foreign english pair counts, and t_ef counts for each pairing

f_count = defaultdict(int)
e_count = defaultdict(int)
fe_count = defaultdict(int)
t_ef = defaultdict(float)

# enumerate through corpus to calculate count of foreign words, english words, and english/foreign word pairings

for (n, (f, e)) in enumerate(bitext):
    for f_i in f:
        f_count[f_i] += 1
        for e_j in e:
            fe_count[(e_j, f_i)] += 1
    for e_j in e:
        e_count[e_j] += 1
    # if n % 500 == 0:
    #     sys.stderr.write(".")

# sys.stderr.write(".")

# initialization of t_ef and number of combinations

NumberOfCombinations = len(fe_count)

for key in fe_count:
    t_ef[key] = 1 / NumberOfCombinations

s_total = {}

# initialize variables used in determining covergence of Model 1

not_converged = True
prev = 0
convergenceSum = 0
numProbabilities = 0

# IBM Model 1 used to determine t_ef values

while not_converged:
    count = defaultdict(float)
    total = defaultdict(float)

    for (n, (f, e)) in enumerate(bitext):
        for e_j in e:
            s_total[e_j] = 0
            for f_i in f:
                s_total[e_j] += t_ef[(e_j, f_i)]
        for e_j in e:
            for f_i in f:
                count[(e_j, f_i)] += t_ef[(e_j, f_i)] / s_total[e_j]
                total[f_i] += t_ef[(e_j, f_i)] / s_total[e_j]
        for f_i in f:
            for e_j in e:
                prev = t_ef[(e_j, f_i)]
                t_ef[(e_j, f_i)] = count[(e_j, f_i)] / total[f_i]
                convergenceSum += abs(prev - t_ef[(e_j, f_i)])
                numProbabilities += 1

    # determine if Model 1 has converged using current and previous t_ef values for every word pairing

    if (convergenceSum / numProbabilities) < 0.002:
        not_converged = False

# returning foreign to english word pairings

for (f, e) in bitext:
  for (i, f_i) in enumerate(f):
    for (j, e_j) in enumerate(e):
      if t_ef[(e_j, f_i)] >= 0.36:
        sys.stdout.write("%i-%i " % (i,j))
  sys.stdout.write("\n")


